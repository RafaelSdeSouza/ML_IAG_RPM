---
title: "Testes com os dados 'Sikora_Complete.txt'"
author: "Allan Vieira"
date: "2017-10-20"
output: 
  html_document:
    keep_md: true
---

<!-- <style> -->
<!-- body { -->
<!-- text-align: justify} -->
<!-- </style> -->

<!-- Esse código de cima para alinha texto não funciona no github -->

Pessoal, procurarei relatar de forma mais detalhada as técnicas que empreguei e as tentativas de verificar a possível separação nos dados, segundo a hipótese apresentada pelo Pedro.

De modo geral, foi possível verificar uma separação dos dados no sentido da hipótese proposta em **apenas** uma única ocasião ao se empregar *Gaussian Mixture Models*. A separação somente se verifica, testando exclusivamente $k = 2$ clusters e com chute inicial específico. Embora não fosse o objetivo inicial, cheguei a tentar outra técnica de clustering, a *DBSCAN*. Os resultados, como vocês poderão ver, foram na mesma linha daqueles ao se empregar *GMM*. Procurei empregar o algoritmo do pacote mclust e também o algoritmo da scikit learn do Python e os resultados foram na mesma direção. De modo geral, foi possível observar que os algoritmos de agrupamento convergiam (grosso modo) para os tipos já estabelecidos dos AGN's: BLRG, FRI PGQs, RLQ e SG-LINERS. A figura abaixo foi confeccionada apenas atribuindo cores aos tipos dos AGN's e servirá de base para vocês compararem com os resultados dos algoritmos de agrupamento.


```{r fig1_tipos, echo=TRUE, fig.width=7, fig.height=5, message=FALSE}
library(ggplot2)
# leitura:
load("~/WKSPCE_sikora.RData")
sikora.df <- read.table(sikora.loc, header=TRUE)

# verificação dos dados:
p1 <- ggplot(sikora.df, aes(Core, B_Band))+
  geom_point(aes(colour = Type), size = 2); p1

```

Passo a agora à exposição dos códigos e resultados.

### Mixtools (k=5)

Inicialmente, empreguei o pacote $mixtools$, mais especificamente a função para clustering via mistura de gaussianas $mvnormalmixEM$. Por default, essa função testa o agrupamento em apenas dois clusters (k=2) com inicialização aleatória do algoritmo. Antes, chamamos a função forçando a classificação em 5 grupos (k=5), para verificar o caso mais geral.

```{r mix1_k5, eval=FALSE, echo=TRUE}
# mixtools
library(mixtools)

# rodando o algoritmo:
#set.seed(1984)
out1 <- mvnormalmixEM(sikora.df[,c(3,1)], k=5) #lento 
```

```{r mix1_k5-plot, fig.width=5, fig.height=4, cache=TRUE, message=FALSE}
# plotando os resultados:
plot(out1, density = TRUE, alpha = c(0.01, 0.05, 0.10, 0.15), which = 2)

```

Apenas definindo k=5, nota-se que as elipses se aproximam (*grosso modo*) dos tipos descritos de AGN's.


### Mclust (sem restrições)

Além do mixtools, usei também o pacote $mclust$. Esse pacote, por default, elenca os melhores modelos e o $k$ número de clusters ideal com base no BIC. Nos gráficos abaixo, nota-se que os melhores resultados e modelos ocorrem com $k=3$, indo ao encontro do que o Rafael já havia verificado numa análise preliminar. 

```{r mclust2_k_livre, eval=TRUE, echo=TRUE, fig.width=5, fig.height=4, message=FALSE}
#MClust
library(mclust)
out2 <- Mclust(sikora.df[,c(3,1)])

library(factoextra)
fviz_mclust(out2, "classification", geom = "point")
fviz_mclust_bic(out2)

```


Cabe ainda uma observação importante: o BIC calculado para dois grupos provavelmente não deve dizer respeito aos dois grupos das diagonais. Basta verificar que rodando o EM no mixtools para $k=2$ clusters, sem forçar o chute inicial, nos retorna partições diametralmente opostas com um grupo bem definido à esquerda e outro à direita. Isso pode ser verificado também no script do Python. Na maioria das vezes essa é a convergência que se obtém ao definir $k=2$ grupos.

```{r mix3_k2, fig.width=5, fig.height=4, message=FALSE}
library(mixtools)
# rodando o algoritmo:
out3 <- mvnormalmixEM(sikora.df[,c(3,1)]) # sem qualquer semente

# plotando os resultados:
plot(out3, density = TRUE, alpha = c(0.01, 0.05, 0.10, 0.15), which = 2)
```

Então, até aqui os resultados parecem indicar que aquela separação na diagonal não seja tão significativa. Mas vamos fazer mais alguns testes.


### Voltando ao mixtools (k=2 e chute inicial)

Sabendo que os resultados do algoritmo EM (que é o algoritmo por trás dos Gaussian Mixture Models) apresenta resultados diferentes para *chutes iniciais* distintos, optei por ir testando diversas sementes e/ou médias para inicialização a fim de verificar se alguma configuração inicial levaria a uma convergência do algoritmo naqueles dois clusters na diagonal. Fato é que apenas com a algumas médias bem específicas (ou semente = 99) isso aconteceu, conforme se constata da figura abaixo. É a mesma que eu enviei por e-mail em outra ocasião. 


```{r mix4_k2_driven, eval=TRUE, echo=TRUE, fig.width=6, fig.height=6, message=FALSE}
# mixtools
library(mixtools)

# rodando o algoritmo:
set.seed(99)
out4 <- mvnormalmixEM(sikora.df[,c(3,1)]) # sem qualquer semente

# plotando os resultados:
plot(out4, density = TRUE, alpha = c(0.01, 0.05, 0.10, 0.15), which = 2)

```

Um gráfico sem as elipses, para vermos melhor o resultado da classificação na diagonal:

``` {r clusterPlots, echo=TRUE, fig.width=5, fig.height=4 , message=FALSE}
# recuperando a classificação:
out4.classes <- as.integer(apply(out4$posterior,1,which.max))

sikora2.df <- data.frame(sikora.df[,c(1,3)], cluster = factor(out4.classes))

library(ggplot2)
clusterPlots.gg <- ggplot((sikora2.df))
clusterPlots.gg + geom_point(aes(x = B_Band, y = Core, color = cluster))
```


Essa convergência também foi conseguida usando o GMM do Python (vide relatório do jupyter-notebook), mas também para configurações muito específicas. Qualquer modificação dos chutes iniciais, os grupos convergem para aqueles separados mais à esquerda e mais à direita, conforme verificamos anteriormente. Ao meu ver essa única convergência, numa configuração demasiadamente específica já indica que provavelmente, em termos de técnicas de clustering, não podemos afirmar que exista aquela separação na diagonal.

Em suma, os mesmos resultados podem ser notados no script implementado em Python no jupyter-notebook: a convergência para os clusters na diagonal somente ocorre com chutes iniciais específicos; qualquer inicialização aleatória do algoritmo para $k=2$ clusters converge para as partições à esquerda e à direita e não na diagonal; definir $k=5$ faz com que o algoritmo convirja para uma situação próxima à classificação conhecida dos 5 grupos de AGN's; o BIC para $k=3$ grupos é mais significativo.


### Outra abordagem

Após essas análises, resolvi me perguntar se, mesmo os algoritmos "puxando" para a classificação conhecida dos AGN's, aquela divisão em dois grupos na diagonal não seria relevante em termos de outros métodos estatísticos - ou seja - vai que sejam subgrupos significativos. Surgiu a ideia então de, a partir daquele agrupamento com chute inicial no mixtools que convergia para grupos na diagonal, aproveitar essa classificação e separar os dados como se fossem duas amostras distintas: cluster 1 - amostra 1 e cluster 2 - amostra 2. O objetivo seria realizar um teste de hipótese multivariado, nos moldes do teste t de Student para diferença de médias. No caso multivariado, o teste seria o teste $T^{2}$ de *Hotelling*. No entanto teríamos que 'passar por cima' de pressupostos de Normalidade em cada grupo (que não existia) e de igualdade da matriz de covariâncias (que também não se verificou). Esses dois problemas seriam até 'manobráveis', uma vez que testes do tipo $"t"$ não são tão sensíveis à violações de normalidade e há uma implementação do teste de Hotelling para o caso de matriz de Covariâncias desiguais.  Mas o maior problema (e incontornável) seria a dependência entre as "amostras". Ao meu ver não poderíamos tratar essas amostras como independentes, uma vez que na verdade elas advém de agrupamentos dentro da mesma amostra. Isso ocasionaria um vicío a favor de rejeitar a hipótese nula de igualdade de médias. De qualquer forma, se alguém tiver uma interpretação diferente, ou alguma informação ou insight que nos permita trabalhar com isso, acho que vale a discussão.

Vou colocar o código abaixo, só como uma indicação do que foi tentado, mas que não se aplicaria. Os resultados foram no sentido de se rejeitar $H_o$ - ou seja - existe diferença significativa entre as médias dos dois grupos na diagonal.

```{r hotelling, eco = TRUE, message=FALSE}

# separando as supostas populações com base no resultado dos clusters do mixtools (k=2, seed=100):

pop1.m <- as.matrix( sikora2.df[sikora2.df$cluster == "1", c(1,2)] )
pop2.m <- as.matrix( sikora2.df[sikora2.df$cluster == "2", c(1,2)] )

## pressupostos para aplicarmos o teste T² de Hotellig
# verificando a normalidade dos grupos - Teste de Mardia (Normalidade multivariada):
library(MVN)

mardiaTest(pop1.m)
mardiaTest(pop2.m)

# verificar se as matrizes de covariâncias podem ser consideradas iguais também:
#Teste de Box Multivariado:
library(heplots)

boxM( cbind(Core, B_Band) ~ cluster, data=sikora2.df)
# há evidências para rejeitar homogeneidade de variâncias

# teste de hotelling:
library(ICSNP)
options(scipen = 999)
HotellingsT2(pop1.m, pop2.m)
# https://us.sagepub.com/sites/default/files/upm-binaries/70364_Schumacker_Chapter_3.pdf
```


### Tentando outro algoritmo de clustering: DBSCAN

Por fim, tentei buscar um outro algoritmo de agrupamento: o DBSCAN. Ele funciona com base em distâncias e tem uma característica de classificar também outliers que, teoricamente, não se encaixariam em nenhum grupo. Os parâmetros dele são bem flexíveis, e conforme podemos ver na figura abaixo, ele é um bom método para reconhecer clusters nesse estilo "diagonal". 

![](http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)
Fonte: http://scikit-learn.org

Testei esse algoritmo com diversos parâmetros. O DBSCAN recebe de entrada um épsilon de distância máxima entre os pontos e mínimo de pontos em cada grupo. Por isso, que alguns pontos ficam de fora e acabam sendo considerados como outliers. Como vocês podem ver abaixo não houve qualquer convergência que apontasse para a existência daquele dois grupos. Coloquei apenas algumas combinaões que foram tentadas.

```{r dbscan_results, echo = TRUE, fig.width=5, fig.height=4, message=FALSE}
library(fpc)

x <- as.matrix(sikora.df[,c(3,1)])

# teste 1:

set.seed(123)
out1.db <- fpc::dbscan(x, eps = 0.4, MinPts = 5)

library("factoextra")
fviz_cluster(out1.db, data = x, stand = FALSE,
             ellipse = FALSE, show.clust.cent = FALSE,
             geom = "point",palette = "jco", ggtheme = theme_classic())

# teste 2:
#set.seed(123)
out2.db <- fpc::dbscan(x, eps = 0.8, MinPts = 7)

fviz_cluster(out2.db, data = x, stand = FALSE,
             ellipse = FALSE, show.clust.cent = FALSE,
             geom = "point",palette = "jco", ggtheme = theme_classic())

# teste 3:
#set.seed(1234)
out3.db <- fpc::dbscan(x, eps = 1.5, MinPts = 4)

fviz_cluster(out3.db, data = x, stand = FALSE,
             ellipse = FALSE, show.clust.cent = FALSE,
             geom = "point",palette = "jco", ggtheme = theme_classic())


# teste 4:
#set.seed(1234)
out4.db <- fpc::dbscan(x, eps = 0.2, MinPts = 8)

fviz_cluster(out4.db, data = x, stand = FALSE,
             ellipse = FALSE, show.clust.cent = FALSE,
             geom = "point",palette = "jco", ggtheme = theme_classic())

```

Portanto, pessoal, em termos de técnicas de clustering, creio não haver nada que indique a existência de dois grupos com aquela configuração diagonal nos dados do "Sikora".
